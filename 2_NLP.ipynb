{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-cutting",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-reflection",
   "metadata": {},
   "source": [
    "### Import NLP Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portuguese-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk as nlp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accompanied-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/winemag-data-130k-v2.csv')\n",
    "df = df[['description', 'title', 'variety']]\n",
    "df = df.dropna()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-intention",
   "metadata": {},
   "source": [
    "Download NLTK Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptionList = list()\n",
    "lemma = nlp.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-protein",
   "metadata": {},
   "source": [
    "Big wait time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polish-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "for description in tqdm(df.description):\n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description) # We use regular expression to delete non-alphabetic characters on data.\n",
    "    description = description.lower() # Since upper and lower characters are (e.g a - A) evaluated like they are different each other by computer we make turn whole characters into lowercase.\n",
    "    description = nltk.word_tokenize(description) # We tokenized the statement\n",
    "    description = [i for i in description if not i in set(stopwords.words(\"english\"))] # We will remove words like 'the', 'or', 'and', 'is' etc.\n",
    "    description = [lemma.lemmatize(i)for i in description] # e.g: loved => love\n",
    "    description = \" \".join(description) # Now we turn our words list into sentence again\n",
    "    descriptionList.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "descDF = pd.DataFrame(descriptionList)\n",
    "descDF.to_csv('data/description_list_description_only_NA_dropped.csv')\n",
    "descDF.to_pickle('data/description_list_description_only_NA_dropped.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-doctor",
   "metadata": {},
   "source": [
    "### Applying NLP on merging title and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controversial-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe9cbf5d92c4595b43e45d94dec1239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/129970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "descriptionList = list()\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "for i in tqdm(range(len(df))):\n",
    "    description = df.title[i] + \". \" + df.description[i]\n",
    "    description = re.sub(\"[^a-zA-Z]\",\" \",description) # We use regular expression to delete non-alphabetic characters on data.\n",
    "    description = description.lower() # Since upper and lower characters are (e.g a - A) evaluated like they are different each other by computer we make turn whole characters into lowercase.\n",
    "    description = nltk.word_tokenize(description) # We tokenized the statement\n",
    "    description = [i for i in description if not i in set(stopwords.words(\"english\"))] # We will remove words like 'the', 'or', 'and', 'is' etc.\n",
    "    description = [lemma.lemmatize(i)for i in description] # e.g: loved => love\n",
    "    description = \" \".join(description) # Now we turn our words list into sentence again\n",
    "    descriptionList.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "descDF = pd.DataFrame(descriptionList)\n",
    "descDF.to_csv('data/description_list_title_desc_NA_dropped.csv')\n",
    "descDF.to_pickle('data/description_list_title_desc_NA_dropped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-major",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
